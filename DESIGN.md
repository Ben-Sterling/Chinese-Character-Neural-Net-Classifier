The goal for this project was to create a python program that identified chinese characters for numbers provided from user drawn input. This project is heavily indebted to and inspired by Jadeep Singh's article Handwritten Digit Recognition GUI App, which provides a tutorial for a python program that identified digits from the MNIST database of handwritten Arabic numerals provided from user drawn input. Without this medium post, I would not have been able to figure out the tkinter gui or the opencv wizardry that made this project possible. 

In order to give a proper explanation of my design choices for this project, I will first explain the process and history of developing this project, and then walk through both code files, explaining at a broad level how they work. 

The first step I took when creating this project was to simply build a convolutional neural net --any convolutional neural net -- because I thought that the CNN would be the most difficult part of the project (incorrectly, I will add) and I wanted to gauge the feasability of this project by seeing if I could build a CNN at all. I was aided in this process by Victor Zhou's guide "Keras for Beginners: Implementing a Convolutional Neural Network" which provided a walkthrough on how to build a CNN that classified images from the aforementioned MNIST database (which by the way, I've learned is effectively the "hello world" of machine learning). Between my blog guide and some guidance from TAs and friends of TAs, I was able to create a working model in jupyter using keras that accurately classified images from the MNIST database. This was a promising step. The next would be to apply the same architecture to classify images from the kaggle database of "Chinese MNIST" images instead. This proved much more difficult than I originally anticipated. TensorFlow has preinstalled in its library a load_data function that handily loads the images from the MNIST library prepocessed into clean numpy arrays pre-split into testing and training data. The kaggle dataset I was attempting to tackle simply had a folder of 15,000 jpeg files and a csv with the labels for those jpeg files. To make matters worse, the sizing and pixel count of the images in the MNIST database and the images in the kaggle database differed greatly. It was effectively an exercise in fitting a square peg in a round hole. All the same, my first priority was to figure out a way to split the data into training and testing sets for neural net. After some research, I found the splitfolders python library, which I then created a python script to split my images into two separate training and testing folders. While figuring out this library, I sort of broke my computer by messing with the folder permissions for some of the folders I was putting out when testing out the script, so now I have several folders that I cannot delete even with admin permissions, but that's neither here nor there. Unfortunately, even after I successfully split the folder of images in two, I had no idea how to match the labels in the csv file to the image's new location. This problem took me a long, long time to solve and I nearly gave up, but I eventually found a way --steal from someone else. As part of the kaggle space, users can submit their own code solutions on how they used the database alongside the database. This was not something I knew when first starting on this project, but proved to be invaluable once I knew in terms of solving this problem. In particular, Richard Kuo and Nikola Bozhinov's submissions, found at https://www.kaggle.com/rkuo2000/chinese-mnist and https://www.kaggle.com/vislupus/cnn-model-chinese-mnist-98-9-accuracy, respectively, proved to be great help. I'll go in more depth later, but the intuition behind their solution was to take advantage of the consistent naming convention of the jpeg files to read in the path to the jpeg files into the same array that housed the csv information, and only then actually split the array, rather than the actual folder of images, which seems obvious in retrospect. With loading the data in the right form taken care of, it was simply a matter of trial and error to change the architecture of the neural net to better match the data. After a bit of experimentation and several office hour queries, I had a functional and trained neural net saved to an h5 file. Armed with this model, I then moved on to the gui. In many ways the gui proved easier than the model. While I had had some previous experience with keras models, I have never used tkinter before and the one time I'd tried to mess with opencv lead me to give up in exasperation. That said, following Jadeep Singh's medium post and the code provided, creating a gui for a user to draw in did not prove difficult. Going from there to a prediction on the content of the input using the trained model proved difficult. Following the tutorial to the letter proved ineffective, and frankly some of the design and geometry proved cryptic, so instead with great help from Cody and some graph paper I was able to redo the geometry for the placement of the blue rectangle around the region of interest, and the screenshot to actually get the image to submit to the model. Once the gui was functional, only then did I realize another problem --my model was garbage, consistently scoring roughly 7% accuracy on test sets despite scoring in the high 90s during training. This is a classic symptom of overfitting the model, and for the life of me I could not figure out how to improve the model within the time frame. This was not for lack of trying. I first lowered the number of epochs and increased the dropout rate, which is to my understanding the standard stop gap fix for overfitting, but none of my changes made a difference. I then tried to change how the data was input into the model using cross-fold validation and dataset augmentation, but I was unable to figure out how to format the data correctly within the time frame. Moving on, I thought that perhaps I had made a mistake in setting up my environment, so I tried using another kaggle user's classification model for the same dataset that claimed a 98.9% accuracy. Strangely, when used locally the testing results varied pretty significantly each time I ran the test. Therefore, I suspect that my model's problem with accuracy is a combination of mistakes made when setting up my environment that I cannot pinpoint, and a suboptimal model architecture, dataset, and fit function. For the former, the only fix would be to either start anew with a brand new environment, or to painstakingly go through each library and path in the current environment. For the latter, KerasTuner is a library that is designed to help find the optimal hyperparameters for a model, and then the aformentioned techniques of cross-fold variation and data augmentation. All the same, given the time frame I decided to accept my current model, although the options outlined would be interesting to investigate further. 

As for the actual code, the FinalModel file begins with importing the various libraries needed. It then goes on to read the csv with the labels into DataFrame. Then, taking advantage of the fact that all the jpeg files have the same naming convention, use this fact to add the file paths to the respective entry in the same dataframe. Now that the images and labels are related, we can use sklearn to split the data into training, testing, and val sets. The ratio for these splits are subjective, although some ratios are better than others. The ones I chose I chose because they seemed reasonable based on what I learned from the original CNN tutorial, although perhaps if I split the data another way I would have gotten better results. The next step is to go from jpeg file path to usable arrays, which is done with skimage. Once accomplished, the inputs are normalized. With the data ready, it's now time to create the model. For my model, I did three layers with node size 32, 64, and 128. These choices are somewhat arbitrary, but theoretically correspond to the 64 by 64 size of the dataset images. The weight for the dropout function is similarly guesswork. Most of the rest of the choices made in the model architecture, however, are dictated by my research (read blogs, youtube videos, and stackexchange exchanges) as for what is necessary for multi-class classification tasks, ie that the loss function is type categorical crossentropy, the optimizer method used the Adam algorithm, the metric for training was accuracy, and the activation function is the standard rectified linear unit. After creating the model, I then train it with the .fit() function, test it with the .evaluate() function, and save it to an h5 file with the .save() functions all available in the tensorflow/keras system. Once the model is saved, the first thing we do in the gui.py file after loading the necessary libraries is loading the model. Afterwords, we define the functions that will help in making the canvas, that is the function for clearing the canvas using the delete() method, the function for listening for mouse button motion using the tkinter library, binding draw lines to the button motion event, and the function for drawing a line on the canvas once detecting that motion. Next function defined is the most difficult, the recognize digit function. This begins with using the ImageGrab module to essentially take a screenshot of the contents of the canvas. We then crop the screenshot and save it as a png. Then we break out the opencv for object recognition. We first read the image in color, then convert the image to black and white, invert the image colors to better match the dataset, and then use the findcontours function to extract the contours of the image. We then create a bounding box using opencv, which is then resized and reshaped to better match the size and shape of the entries into the kaggle dataset. Finally we call the model using our new image to try to classifiy it, and then print the result again using opencv. After defining all of the functions, it's now time to call them, using tkinter to create the main window and then mainloop() to loop through the code. 


